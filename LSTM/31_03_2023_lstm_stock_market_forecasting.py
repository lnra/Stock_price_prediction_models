# -*- coding: utf-8 -*-
"""31_03_2023_LSTM-Stock-Market-Forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lItjMQW5TkCtBtuegEO7Qj343S2DuA8y

#Part 1: LSTM

## Import libraries
"""

# Required to save models in HDF5 format
!pip install h5py

# Install and update required packages for Python 3.9
!sudo apt update
!sudo apt install python3.9

# Import required libraries for data analysis, visualization, 
# and machine learning
import pandas as pd
import pandas_datareader as pdr
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
import numpy as np
from numpy import array
import math
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import LSTM

# The above libraries are used for various purposes in the code:
# pandas: for data manipulation and analysis
# pandas_datareader: for fetching data from Yahoo Finance
# matplotlib: for data visualization
# plotly: for interactive data visualization
# numpy: for numerical computing
# sklearn: for data preprocessing and evaluation
# tensorflow and keras: for building and training the LSTM model

"""## Data Collection"""

# Import the YahooFinance API and datetime library

import yfinance as yf
from datetime import datetime 

# Define the stock symbol and start date for data collection
symbol= 'TSLA'
start_date = datetime(2018, 1, 1)

# Download data from YahooFinance API for the given stock symbol and start date
df = yf.download(symbol, start=start_date)


# Save the downloaded data to a CSV file
df.to_csv('TSLA.csv')

# Read the saved CSV file into a Pandas DataFrame for further analysis
df=pd.read_csv('TSLA.csv')

# This code displays the first few rows of a pandas DataFrame 'df'
# It is used to check if the data has been loaded correctly and to understand 
# the structure of the data
# By default, it displays the first 5 rows of the DataFrame
df.head()

# Display the last 5 rows of the DataFrame to check the latest data
df.tail()

# We select only the 'Date' and 'Close' columns from our dataframe
# and make a copy of it
df = df[['Date', 'Close']].copy()

# The 'Date' column is in the object type, so we convert it to a string
# and remove the last 14 digits,
# which represent the timestamp, to keep only the date
df['date'] = df['Date'].astype(str).str[:-14]
print(df)

# The describe() function is used to view some basic statistical details 
# like count, mean, std dev, min, max, etc. of a dataframe.
# It provides a summary of the central tendency, dispersion and shape of 
# the distribution of a dataset.

df.describe()

"""## Analysis of Year 2018






"""

# Convert 'Date' column to datetime format with year-month-day format
df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')

# Filter data for year 2018 and store it in y_2018 variable
y_2018 = df.loc[(df['Date'] >= '2018-01-01')
                     & (df['Date'] < '2019-01-01')]

y_2018

# Group daily closing prices by month for the year 2018
monthly= y_2018.groupby(y_2018['Date'].dt.strftime('%B'))[['Close']].mean()

# Define the order of the months
order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 
             'September', 'October', 'November', 'December']

# Reindex the monthly dataframe to match the specified order             
monthly = monthly.reindex(order, axis=0)

# Display the monthly average closing prices for 2018
monthly

# Create an empty plotly figure
fig = go.Figure()

# Add a bar trace with monthly index as x and monthly 'Close' as y
fig.add_trace(go.Bar(
    x=monthly.index,
    y=monthly['Close'],
    name='Stock Close Price',
    marker_color='rgb(133, 65, 181)'
))

# Update the layout of the plot, rotate x-axis labels by -45 degrees and set a title
fig.update_layout(barmode='group', xaxis_tickangle=-45, 
                  title='Monthly comparision of close prices - 2018')
# Display the plotly figure
fig.show()

"""## Analysis of Year 2019"""

year_2019 = df.loc[(df['Date'] >= '2019-01-01')
                   & (df['Date'] <= '2020-1-1')]

monthly= year_2019.groupby(year_2019['Date'].dt.strftime('%B'))[['Close']].mean()
order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 
             'September', 'October', 'November', 'December']
monthly = monthly.reindex(order, axis=0)
monthly

import plotly.graph_objects as go
import plotly.express as px

fig = go.Figure()

fig.add_trace(go.Bar(
    x=monthly.index,
    y=monthly['Close'],
    name='Stock Close Price',
    marker_color='rgb(133, 65, 181)'
))

fig.update_layout(barmode='group', xaxis_tickangle=-45, 
                  title='Monthly comparision of close prices - 2019')
fig.show()

"""## Analysis of Year 2020"""

year_2020 = df.loc[(df['Date'] >= '2020-01-01')
                   & (df['Date'] <= '2021-01-01')]

monthly= year_2020.groupby(year_2020['Date'].dt.strftime('%B'))[['Close']].mean()
order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 
             'September', 'October', 'November', 'December']
monthly = monthly.reindex(order, axis=0)
monthly

import plotly.graph_objects as go
import plotly.express as px

fig = go.Figure()

fig.add_trace(go.Bar(
    x=monthly.index,
    y=monthly['Close'],
    name='Stock Close Price',
    marker_color='rgb(133, 65, 181)'
))

fig.update_layout(barmode='group', xaxis_tickangle=-45, 
                  title='Monthly comparision of close prices - 2020')
fig.show()

"""## Analysis of Year 2021"""

year_2021 = df.loc[(df['Date'] >= '2021-01-01')
                   & (df['Date'] <= '2022-01-01')]

monthly= year_2021.groupby(year_2021['Date'].dt.strftime('%B'))[['Close']].mean()
order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 
             'September', 'October', 'November', 'December']
monthly = monthly.reindex(order, axis=0)
monthly

import plotly.graph_objects as go
import plotly.express as px

fig = go.Figure()

fig.add_trace(go.Bar(
    x=monthly.index,
    y=monthly['Close'],
    name='Stock Close Price',
    marker_color='rgb(133, 65, 181)'
))

fig.update_layout(barmode='group', xaxis_tickangle=-45, 
                  title='Monthly comparision of close prices - 2021')
fig.show()

"""## Analysis of Year 2022"""

year_2022 = df.loc[(df['Date'] >= '2022-01-01')
                   & (df['Date'] <= '2023-01-01')]

monthly= year_2022.groupby(year_2022['Date'].dt.strftime('%B'))[['Close']].mean()
order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 
             'September', 'October', 'November', 'December']
monthly = monthly.reindex(order, axis=0)
monthly

import plotly.graph_objects as go
import plotly.express as px

fig = go.Figure()

fig.add_trace(go.Bar(
    x=monthly.index,
    y=monthly['Close'],
    name='Stock Close Price',
    marker_color='rgb(133, 65, 181)'
))

fig.update_layout(barmode='group', xaxis_tickangle=-45, 
                  title='Monthly comparision of close prices - 2022')
fig.show()

"""## Analysis of Year 2023"""

year_2023 = df.loc[(df['Date'] >= '2023-01-01')
                   & (df['Date'] <= '2023-12-31')]

monthly= year_2023.groupby(year_2023['Date'].dt.strftime('%B'))[['Close']].mean()
order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 
             'September', 'October', 'November', 'December']
monthly = monthly.reindex(order, axis=0)
monthly

import plotly.graph_objects as go
import plotly.express as px

fig = go.Figure()

fig.add_trace(go.Bar(
    x=monthly.index,
    y=monthly['Close'],
    name='Stock Close Price',
    marker_color='rgb(133, 65, 181)'
))

fig.update_layout(barmode='group', xaxis_tickangle=-45, 
                  title='Monthly comparision of close prices -2023')
fig.show()

"""## Lag Plot of Time Series Data for Overall price data"""

# Set the figure size and title
plt.figure(figsize=(18,15))
plt.suptitle('Lag Plots', fontsize=22)

# Add subplots for weekly, monthly, and yearly lags
plt.subplot(3,3,1)
pd.plotting.lag_plot(df['Close'], lag=7) #Weekly Lag
plt.title('Weekly Lag')

plt.subplot(3,3,2)
pd.plotting.lag_plot(df['Close'], lag=31) #Monthly Lag
plt.title('Monthly Lag')


plt.subplot(3,3,3)
pd.plotting.lag_plot(df['Close'], lag=365) #Yearly Lag
plt.title('Yearly Lag')

# Remove legend and show the plot
plt.legend([""], fontsize="large")
plt.show()

"""##Grouping the data by date with mean value of Close Prices"""

# Group the data in the DataFrame by 'Date' column
group = df.groupby('Date')

# Calculate the mean of 'Close' column for each group
df_groupby_date = group['Close'].mean()

# The resulting Series object 'df_groupby_date' contains the average closing price
# for each date in the original DataFrame 'df'.

df_groupby_date.head()

len(df_groupby_date)

"""## Splitting data"""

# Define number of days for prediction
prediction_days = 100

# Set Train data to be all data except last prediction_days
df_train= df_groupby_date[:len(df_groupby_date)-prediction_days].values.reshape(-1,1)

# Set Test data to be last prediction_days
df_test= df_groupby_date[len(df_groupby_date)-prediction_days:].values.reshape(-1,1)

df_train.shape

df_test.shape

"""## Scale Test and Training data"""

# MinMaxScaler scales all the data in the range of 0 to 1
scaler_train = MinMaxScaler(feature_range=(0,1))
scaled_train = scaler_train.fit_transform(df_train)

scaler_test = MinMaxScaler(feature_range=(0,1))
scaled_test = scaler_test.fit_transform(df_test)

# Scaled_train and Scaled_test are numpy arrays now.

scaled_train

scaled_test

# Define a function to generate datasets for LSTM modeling
def dataset_generator_lstm(dataset, look_back=30):
    # Initialize empty lists for input and output data
    dataX, dataY = [], []
    
    # Iterate through the dataset up to (length - look_back) index
    for i in range(len(dataset) - look_back):
        # Select the window of look_back size as input
        window_size_x = dataset[i:(i + look_back), 0]
        
        # Select the next value as output
        dataY.append(dataset[i + look_back, 0])
        
        # Append the input window to the dataX list
        dataX.append(window_size_x)
    
    # Convert dataX and dataY into numpy arrays and return
    return np.array(dataX), np.array(dataY)

# Generate training datasets using the dataset_generator_lstm function
trainX, trainY = dataset_generator_lstm(scaled_train)

# Generate testing datasets using the dataset_generator_lstm function
testX, testY = dataset_generator_lstm(scaled_test)

# printing train and test datasets
print("trainX: ", trainX)
print("\ntrainY: ", trainY)
print("\ntestX: ", testX)
print("\ntestY: ", testY)

# Print the shape of each dataset to check the number of rows and columns

print("trainX: ", trainX.shape)
print("trainY: ", trainY.shape)
print("\ntestX: ", testX.shape)
print("testY: ", testY.shape)

trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))

testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))

print("Shape of trainX: ", trainX.shape)
print("Shape of testX: ", testX.shape)

# Define the model architecture
model = Sequential()

# Adding the first LSTM layer and some Dropout regularization

model.add(LSTM(units = 128, activation = 'relu',return_sequences=True, input_shape = (trainX.shape[1], trainX.shape[2])))

model.add(Dropout(0.2))

# Adding second LSTM layer and some Dropout regularization
model.add(LSTM(units = 64, input_shape = (trainX.shape[1], trainX.shape[2])))
model.add(Dropout(0.2))

# Adding a dense output layer
model.add(Dense(units = 1))

# Printing the model summary
model.summary()

# Importing necessary callbacks from Keras
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

#Compiling the LSTM
model.compile(optimizer = 'adam', loss = 'mean_squared_error',metrics=['accuracy'])

# Define callbacks to have more control over the training process. These include:
# stopping training when a certain accuracy/loss score is reached
# saving the model as a checkpoint after each successful epoch
# adjusting the learning rates over time
# restoring the weights with the best loss score
# The ModelCheckpoint callback saves the model's weights in a file specified 
# in filepath argument.
# The EarlyStopping callback stops training when there is no improvement 
# in the validation loss after a certain number of epochs specified in the patience argument.

checkpoint_path = '/Users/el/Downloads/project_model_final.hdf5'

checkpoint = ModelCheckpoint(filepath=checkpoint_path,
                            monitor='val_loss',
                            verbose=1,
                            save_best_only=True,
                            mode='min')

earlystopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)

callbacks = [checkpoint, earlystopping]

# Fit the model to the training data, and validate it on the test data.
# Use the callbacks to save the best model weights and stop training early if needed.

history = model.fit(trainX, trainY, batch_size = 32, epochs = 250, verbose=1, shuffle=False, validation_data=(testX, testY), callbacks=callbacks)

"""## Training Loss and Testing Loss"""

plt.figure(figsize=(16,7))

plt.plot(history.history['loss'], label='train')

plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

"""## Training Accuracy and Testing Accuracy"""

plt.figure(figsize=(16,7))

plt.plot(history.history['accuracy'], label='train')

plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

"""# LSTM Predictions using testX and plotting line graph against actual testY"""

# Transformation to original form and making predictions
from tensorflow.keras.models import load_model

# loading the saved model from the checkpoint path
model_from_saved_checkpoint = load_model(checkpoint_path)

# making predictions on the test data using the loaded model
predicted_test_data = model_from_saved_checkpoint.predict(testX)

# transforming the predicted data back to its original scale
predicted_test_data = scaler_test.inverse_transform(predicted_test_data.reshape(-1,1))

# transforming the actual test data back to its original scale
test_actual = scaler_test.inverse_transform(testY.reshape(-1,1))

plt.figure(figsize=(16,7))

plt.plot(predicted_test_data, 'r', marker='.', label='Predicted Test')

plt.plot(test_actual, marker='.', label='Actual Test')
plt.legend()
plt.show()

"""# LSTM Predictions using trainX and plotting line graph against actual trainY"""

# Transformation to original form and making predictions

predicted_train_data = model_from_saved_checkpoint.predict(trainX)

predicted_train_data = scaler_train.inverse_transform(predicted_train_data.reshape(-1,1))

train_actual = scaler_train.inverse_transform(trainY.reshape(-1,1))

plt.figure(figsize=(16,7))

plt.plot(predicted_train_data, 'r', marker='.', label='Predicted Train')

plt.plot(train_actual, marker='.', label='Actual Train')
plt.legend()
plt.show()

"""# Calculating Root Mean Squared Error

Root Mean Squared Error (RMSE) is a commonly used metric to evaluate the performance of a regression model in Python. It measures the difference between the predicted values and actual values in a dataset, by taking the square root of the average of the squared differences. RMSE is a popular evaluation metric because it gives a more accurate and meaningful estimate of the model's error compared to the mean absolute error. RMSE is calculated as:

RMSE = sqrt(mean_squared_error(y_true, y_pred))

where y_true is the array of actual values and y_pred is the array of predicted values.
"""

rmse_lstm_test = math.sqrt(mean_squared_error(test_actual, predicted_test_data))

print('Test RMSE: %.3f' %rmse_lstm_test)

rmse_lstm_train = math.sqrt(mean_squared_error(train_actual, predicted_train_data))

print('Train RMSE: %.3f' %rmse_lstm_train)

"""# Prediction of future 30 days price against actual testY"""

testX

testX.shape

lookback_period = 30

testX_last_30_days = testX[testX.shape[0] - lookback_period : ]
testX_last_30_days.shape

testX_last_30_days

predicted_30_days_forecast_price_test_x = []

# loop through the next 30 days and make predictions using the saved model
for i in range(30):
    predicted_forecast_price_test_x = model_from_saved_checkpoint.predict(testX_last_30_days[i:i+1])
    
  # inverse scaling to get the actual forecasted price
predicted_forecast_price_test_x = scaler_test.inverse_transform(predicted_forecast_price_test_x.reshape(-1,1))
# append each predicted price to the list
predicted_30_days_forecast_price_test_x.append(predicted_forecast_price_test_x)

# print the array of predicted prices for the next 30 days  
print("Forecast for the next 30 Days Beyond the actual trading days ", np.array(predicted_30_days_forecast_price_test_x))

predicted_30_days_forecast_price_test_x = np.array(predicted_30_days_forecast_price_test_x)
predicted_30_days_forecast_price_test_x.shape

predicted_30_days_forecast_price_test_x

predicted_30_days_forecast_price_test_x=predicted_30_days_forecast_price_test_x.flatten()

predicted_30_days_forecast_price_test_x

predicted_30_days_forecast_price_test_x.shape

predicted_test_data.shape

predicted_test_data=predicted_test_data.flatten()

predicted_test_data

predicted_30_days_forecast_prices_final = np.concatenate((predicted_test_data, predicted_30_days_forecast_price_test_x))
                                                         
predicted_30_days_forecast_prices_final

predicted_30_days_forecast_prices_final.shape

plt.figure(figsize=(12,5))

plt.plot(predicted_30_days_forecast_prices_final, 'r', marker='.', label='Predicted Test')

plt.plot(predicted_test_data, marker='.', label='Actual test')

plt.legend()
plt.show()

"""predicted_30_days_forecast_price_test_x"""

forecast_30_days_prediction_values = predicted_30_days_forecast_prices_final[-30:]

forecast_30_days_prediction_values

test_30_days_values = test_actual[-30:]

test_30_days_values

plt.figure(figsize=(12,5))

plt.plot(forecast_30_days_prediction_values, 'r', marker='x', label='Predicted Values')

plt.plot(test_30_days_values, marker='.', label='Test Values')

plt.legend()
plt.show()

import datetime as helloworld
from datetime import timedelta
from datetime import date

dates = []
current_date=helloworld.date.today()
d = current_date-timedelta(days=29)
while d <= current_date:
    dates.append(helloworld.datetime.strftime(d,'%Y-%m-%d'))
    d += helloworld.timedelta(days=1)

print(len(dates))

dates

predictions_list = pd.DataFrame(np.column_stack([dates, forecast_30_days_prediction_values, test_30_days_values]), 
                               columns=['Date', 'Predicted 30 Days Prices', 'Test 30 Days Prices'])
predictions_list

"""## Predicting next 30 days"""

len(predicted_test_data)

predicted_test_data = np.array(predicted_test_data)
predicted_test_data.shape

predicted_test_data = predicted_test_data.flatten()
predicted_test_data

x_input =predicted_test_data[-30:].reshape(1,-1)
x_input.shape

temp_input=list(x_input)
temp_input=temp_input[0].tolist()

temp_input

len(temp_input)

# demonstrate prediction for next 30 days
from numpy import array

output = []
n_steps= 30
i = 0

# Define the scaler object used for normalization
scaler = MinMaxScaler()

# Reshape temp_input to have a single feature
temp_input = array(temp_input).reshape(-1, 1)

# Fit the scaler object to your data
scaler.fit(temp_input)

while i < 30:
    if len(temp_input) > 30:
        x_input = array(temp_input[1:])
        x_input = x_input.reshape(-1, 1)  # Reshape to have a single feature
        x_input = scaler.transform(x_input)  # Apply scaling to the input
        x_input = x_input.reshape((1, n_steps, 1))
        yhat = model.predict(x_input, verbose=0)
        yhat = scaler.inverse_transform(yhat)  # Apply inverse scaling to the output
        temp_input = np.append(temp_input, yhat[0])
        temp_input = temp_input[1:]
        output.extend(yhat.tolist())
        i += 1
    else:
        x_input = temp_input.reshape((1, n_steps, 1))
        yhat = model.predict(x_input, verbose=0)
        yhat = scaler.inverse_transform(yhat)  # Apply inverse scaling to the output
        temp_input = np.append(temp_input, yhat[0])
        output.extend(yhat.tolist())
        i += 1

print(output)

predictions = np.array(output)
predictions.shape

prediction= predictions.flatten()
prediction

import pandas as pd
from datetime import date, timedelta

# Create a list of dates for the next 30 days
date_list = [date.today() + timedelta(days=x) for x in range(1, 31)]

# Create a dataframe with the dates and array values
df = pd.DataFrame({'Date': date_list, 'Value': prediction})

# Print the dataframe
print(df)

new_pred_plot = pd.DataFrame({
    'dates': df['Date'],
    'predicted price':df['Value']
})

jsonresult  = new_pred_plot.to_json(orient='records')

jsonresult

import json
with open('data.json', 'w') as f:
    json.dump(jsonresult, f)